{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path('/Users/jobvisser/repos/deep-philosopher/')\n",
    "path = Path('/home/jupyter/deep-philosopher/')\n",
    "stor_path = path / 'app'\n",
    "book = epub.read_epub(path/'app/data/descartes1641.epub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Brackets] enclose editorial explanations. Sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In his title for this work, Descartes is follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some years ago I was struck by how many false ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can do this without showing that all my beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whatever I have accepted until now as most tru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [Brackets] enclose editorial explanations. Sma...\n",
       "1  In his title for this work, Descartes is follo...\n",
       "2  Some years ago I was struck by how many false ...\n",
       "3  I can do this without showing that all my beli...\n",
       "4  Whatever I have accepted until now as most tru..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_par = []\n",
    "\n",
    "for doc in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "    if doc.is_chapter():\n",
    "        chap_raw = doc.get_content()\n",
    "        soup = BeautifulSoup(chap_raw)\n",
    "        for par in soup.find_all('p', text=False, recursive=True):\n",
    "            # filter out paragraphs with less then 50 characters\n",
    "            if len(par.text) > 50:     \n",
    "                try:\n",
    "                    all_par = all_par + [par.text]\n",
    "                except:\n",
    "                    print(f'Not parsable: {par}')\n",
    "    \n",
    "df_texts = pd.DataFrame(all_par[1::], columns=['text'])\n",
    "df_texts.shape[0]\n",
    "df_texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In his title for this work, Descartes is following a tradition (started by Aristotle) which uses ‘first philosophy’ as a label for metaphysics.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model\n",
    "bs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/fastai/data_block.py:215: UserWarning: `random_split_by_pct` is deprecated, please use `split_by_rand_pct`.\n",
      "  warn(\"`random_split_by_pct` is deprecated, please use `split_by_rand_pct`.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = (TextList.from_df(df_texts, stor_path, cols=['text'])\n",
    "            .random_split_by_pct(.1)\n",
    "            .label_for_lm()\n",
    "            .databunch(bs=bs))\n",
    "\n",
    "data_lm.save(stor_path/'static/data_lm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(stor_path/'static', 'data_lm.pkl', bs=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>. . . . xxunk the xxunk of a xxunk xxunk that seems to present more difficulty than it is xxunk . xxbos xxmaj in his xxunk for this work , xxmaj descartes is following a xxunk ( started by xxmaj xxunk ) which uses ‘ first xxunk ’ as a xxunk for xxunk . xxbos xxmaj some xxunk ago i was xxunk by how many false things i had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>xxunk this piece of xxunk in my hands , and so on . xxmaj it seems to be quite impossible to doubt beliefs like these , which come from the senses . xxmaj another example : how can i doubt that these hands or this whole body are mine ? xxmaj to doubt such things i would have to xxunk myself to brain - xxunk xxunk who are convinced they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>something completely xxunk and xxunk — not xxunk like anything ever seen before — at least the colours used in the picture must be real . xxmaj similarly , although these general kinds of things — eyes , head , hands and so on — could be imaginary , there is no xxunk that certain even xxunk and more universal kinds of things are real . xxmaj these are the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>long xxunk of causes and xxunk . xxmaj but the less powerful they make my xxunk cause , the more likely it is that i am so imperfect as to be deceived all the time — because deception and error seem to be xxunk . xxmaj having no answer to these arguments , i am xxunk back to the position that doubts can properly be xxunk about any of my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>if he had one firm and xxunk point he could xxunk the world · with a long enough xxunk · ; so i too can hope for great things if i xxunk to find just one little thing that is xxunk and certain . xxbos i will suppose , then , that everything i see is xxunk . i will believe that my memory tells me nothing but xxunk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>this ‘ i ’ is , i shall go back and think some more about what i believed myself to be before i started this meditation . i will xxunk from those beliefs anything that could be even xxunk called into question by the arguments i have been using , which will xxunk me with only beliefs about myself that are certain and xxunk . xxbos xxmaj well , then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>that i am ? xxmaj can i now xxunk to have any of the features that i used to think belong to a body ? xxmaj when i think about them really carefully , i find that they are all open to doubt : i xxunk n’t xxunk time by showing this about each of them xxunk . xxmaj now , what about the features that i xxunk to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>xxunk ; so it ca n’t depend on anything that i invent in my imagination . xxmaj the word ‘ invent ’ xxunk to what is wrong with xxunk on my imagination in this matter : if i used imagination to show that i was something or other , that would be mere xxunk , mere xxunk - xxunk ; for imagining is simply contemplating the shape or image of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>say for sure that i now see the flames , hear the xxunk xxunk , and feel the heat of the fire ; but i certainly seem to see , to hear , and to be xxunk . xxmaj this can not be false ; what is called ‘ xxunk ’ is strictly just this xxunk , and when ‘ xxunk ’ is understood in this xxunk sense of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>xxunk it was not any of the features that the senses xxunk me of ; for all of them — brought to me through taste , smell , sight , touch or hearing — have now xxunk , yet it is still the same wax . xxbos xxmaj perhaps what i now think about the wax xxunk what its nature was all xxunk . xxmaj if that is right ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch(rows=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " ',',\n",
       " 'i',\n",
       " 'that',\n",
       " 'the',\n",
       " '.',\n",
       " 'of']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the most common dictionary terms\n",
    "data_lm.vocab.itos[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Language Model\n",
    "# drop_mult is a parameter that controls the % of drop-out used\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='12' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      92.31% [12/13 00:09<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.209612</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.211710</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.206813</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.206625</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.206575</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.201503</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.175935</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.109891</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.980600</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.810596</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.820326</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.633545</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      37.50% [3/8 00:00<00:00 7.8680]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "# use learning rate finder to identify a good learning rate to use\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.191718</td>\n",
       "      <td>3.806524</td>\n",
       "      <td>0.245536</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.147721</td>\n",
       "      <td>3.713983</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.088407</td>\n",
       "      <td>3.616575</td>\n",
       "      <td>0.271726</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.016553</td>\n",
       "      <td>3.541648</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.931158</td>\n",
       "      <td>3.471267</td>\n",
       "      <td>0.288690</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.837974</td>\n",
       "      <td>3.419085</td>\n",
       "      <td>0.290476</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.734998</td>\n",
       "      <td>3.360668</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.629359</td>\n",
       "      <td>3.313789</td>\n",
       "      <td>0.291964</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.514880</td>\n",
       "      <td>3.296511</td>\n",
       "      <td>0.292560</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.402521</td>\n",
       "      <td>3.292879</td>\n",
       "      <td>0.292560</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.291137</td>\n",
       "      <td>3.285718</td>\n",
       "      <td>0.286012</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.183680</td>\n",
       "      <td>3.313872</td>\n",
       "      <td>0.287202</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.076321</td>\n",
       "      <td>3.300980</td>\n",
       "      <td>0.290774</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.973992</td>\n",
       "      <td>3.328669</td>\n",
       "      <td>0.290179</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.873849</td>\n",
       "      <td>3.355896</td>\n",
       "      <td>0.291964</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.775686</td>\n",
       "      <td>3.409868</td>\n",
       "      <td>0.279762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.681917</td>\n",
       "      <td>3.418390</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.598175</td>\n",
       "      <td>3.440807</td>\n",
       "      <td>0.281548</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.514436</td>\n",
       "      <td>3.486363</td>\n",
       "      <td>0.277381</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.435678</td>\n",
       "      <td>3.505387</td>\n",
       "      <td>0.270536</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.358122</td>\n",
       "      <td>3.520469</td>\n",
       "      <td>0.274107</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.284981</td>\n",
       "      <td>3.561492</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.216338</td>\n",
       "      <td>3.578918</td>\n",
       "      <td>0.263988</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.151757</td>\n",
       "      <td>3.601896</td>\n",
       "      <td>0.269345</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.094264</td>\n",
       "      <td>3.635939</td>\n",
       "      <td>0.263393</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.038618</td>\n",
       "      <td>3.655193</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.986914</td>\n",
       "      <td>3.648064</td>\n",
       "      <td>0.267560</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.941842</td>\n",
       "      <td>3.699073</td>\n",
       "      <td>0.259226</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.901175</td>\n",
       "      <td>3.704205</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.861639</td>\n",
       "      <td>3.726743</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.825370</td>\n",
       "      <td>3.712216</td>\n",
       "      <td>0.254464</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.793975</td>\n",
       "      <td>3.721551</td>\n",
       "      <td>0.260119</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.770480</td>\n",
       "      <td>3.749305</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.744245</td>\n",
       "      <td>3.724376</td>\n",
       "      <td>0.266369</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.724519</td>\n",
       "      <td>3.742642</td>\n",
       "      <td>0.259524</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.705034</td>\n",
       "      <td>3.754002</td>\n",
       "      <td>0.257738</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.685748</td>\n",
       "      <td>3.764945</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.671097</td>\n",
       "      <td>3.748055</td>\n",
       "      <td>0.259226</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.658368</td>\n",
       "      <td>3.762215</td>\n",
       "      <td>0.254167</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.645117</td>\n",
       "      <td>3.762748</td>\n",
       "      <td>0.259524</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# as a rule of thumb, review the plot above and choose the learning rate with the steepest slope to fit the model\n",
    "learn.fit_one_cycle(40, 1e-2, moms=(0.8,0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(stor_path/'models/deep_philosopher_head')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(stor_path/'models/deep_philosopher_head');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.608582</td>\n",
       "      <td>3.775159</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.601264</td>\n",
       "      <td>3.774205</td>\n",
       "      <td>0.255060</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.581975</td>\n",
       "      <td>3.810191</td>\n",
       "      <td>0.252976</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.558100</td>\n",
       "      <td>3.848574</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.543243</td>\n",
       "      <td>3.884786</td>\n",
       "      <td>0.253274</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.521868</td>\n",
       "      <td>3.904082</td>\n",
       "      <td>0.251488</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.492076</td>\n",
       "      <td>3.948315</td>\n",
       "      <td>0.251190</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.461621</td>\n",
       "      <td>3.991401</td>\n",
       "      <td>0.246429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.426844</td>\n",
       "      <td>4.010464</td>\n",
       "      <td>0.258631</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.393766</td>\n",
       "      <td>4.051111</td>\n",
       "      <td>0.247321</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.353578</td>\n",
       "      <td>4.082736</td>\n",
       "      <td>0.242262</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.304145</td>\n",
       "      <td>4.159216</td>\n",
       "      <td>0.243155</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.251840</td>\n",
       "      <td>4.240019</td>\n",
       "      <td>0.245833</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.201676</td>\n",
       "      <td>4.307986</td>\n",
       "      <td>0.232738</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.140355</td>\n",
       "      <td>4.401661</td>\n",
       "      <td>0.233929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.077399</td>\n",
       "      <td>4.455857</td>\n",
       "      <td>0.236905</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.014374</td>\n",
       "      <td>4.454490</td>\n",
       "      <td>0.233631</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.949968</td>\n",
       "      <td>4.529709</td>\n",
       "      <td>0.233929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.884532</td>\n",
       "      <td>4.610864</td>\n",
       "      <td>0.230655</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.834287</td>\n",
       "      <td>4.666089</td>\n",
       "      <td>0.227381</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.770554</td>\n",
       "      <td>4.682059</td>\n",
       "      <td>0.230060</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.714004</td>\n",
       "      <td>4.669415</td>\n",
       "      <td>0.237202</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.653089</td>\n",
       "      <td>4.724329</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.599145</td>\n",
       "      <td>4.770562</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.550554</td>\n",
       "      <td>4.716942</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.510058</td>\n",
       "      <td>4.715287</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.473461</td>\n",
       "      <td>4.713126</td>\n",
       "      <td>0.233036</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>4.703025</td>\n",
       "      <td>0.232440</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.408827</td>\n",
       "      <td>4.763349</td>\n",
       "      <td>0.222024</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.377694</td>\n",
       "      <td>4.738903</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.352706</td>\n",
       "      <td>4.777035</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.331072</td>\n",
       "      <td>4.674246</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.310211</td>\n",
       "      <td>4.697463</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.290881</td>\n",
       "      <td>4.735643</td>\n",
       "      <td>0.230060</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.272383</td>\n",
       "      <td>4.671063</td>\n",
       "      <td>0.230655</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.255527</td>\n",
       "      <td>4.650858</td>\n",
       "      <td>0.232440</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.240810</td>\n",
       "      <td>4.659513</td>\n",
       "      <td>0.234524</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.227644</td>\n",
       "      <td>4.610247</td>\n",
       "      <td>0.234226</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.217381</td>\n",
       "      <td>4.677680</td>\n",
       "      <td>0.233036</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.212957</td>\n",
       "      <td>4.660922</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.205370</td>\n",
       "      <td>4.632873</td>\n",
       "      <td>0.236310</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.200635</td>\n",
       "      <td>4.644148</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.197012</td>\n",
       "      <td>4.741986</td>\n",
       "      <td>0.223512</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.190667</td>\n",
       "      <td>4.666144</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.183676</td>\n",
       "      <td>4.655231</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.179294</td>\n",
       "      <td>4.637728</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.181504</td>\n",
       "      <td>4.767141</td>\n",
       "      <td>0.222321</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.182597</td>\n",
       "      <td>4.688720</td>\n",
       "      <td>0.234524</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.181353</td>\n",
       "      <td>4.714253</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.176401</td>\n",
       "      <td>4.668826</td>\n",
       "      <td>0.227976</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.171009</td>\n",
       "      <td>4.633759</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.165008</td>\n",
       "      <td>4.615773</td>\n",
       "      <td>0.233929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.163242</td>\n",
       "      <td>4.661421</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.161471</td>\n",
       "      <td>4.663189</td>\n",
       "      <td>0.230952</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.157317</td>\n",
       "      <td>4.694508</td>\n",
       "      <td>0.224702</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.152712</td>\n",
       "      <td>4.643038</td>\n",
       "      <td>0.228274</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.147406</td>\n",
       "      <td>4.620551</td>\n",
       "      <td>0.232440</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.145287</td>\n",
       "      <td>4.603698</td>\n",
       "      <td>0.230357</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.143183</td>\n",
       "      <td>4.610306</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.141134</td>\n",
       "      <td>4.593605</td>\n",
       "      <td>0.235119</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.137925</td>\n",
       "      <td>4.653940</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.134974</td>\n",
       "      <td>4.598501</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.133948</td>\n",
       "      <td>4.613222</td>\n",
       "      <td>0.232440</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.131099</td>\n",
       "      <td>4.643037</td>\n",
       "      <td>0.226488</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.130785</td>\n",
       "      <td>4.636984</td>\n",
       "      <td>0.226488</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.128707</td>\n",
       "      <td>4.617088</td>\n",
       "      <td>0.233036</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.131225</td>\n",
       "      <td>4.602130</td>\n",
       "      <td>0.231845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.129113</td>\n",
       "      <td>4.589922</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.128011</td>\n",
       "      <td>4.607652</td>\n",
       "      <td>0.228274</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.126324</td>\n",
       "      <td>4.621565</td>\n",
       "      <td>0.225298</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.125391</td>\n",
       "      <td>4.593310</td>\n",
       "      <td>0.234821</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.126819</td>\n",
       "      <td>4.581158</td>\n",
       "      <td>0.236310</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>4.627912</td>\n",
       "      <td>0.231548</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.126571</td>\n",
       "      <td>4.610700</td>\n",
       "      <td>0.232440</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.125576</td>\n",
       "      <td>4.618590</td>\n",
       "      <td>0.231845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.123587</td>\n",
       "      <td>4.596449</td>\n",
       "      <td>0.233929</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.125434</td>\n",
       "      <td>4.592216</td>\n",
       "      <td>0.233036</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.127604</td>\n",
       "      <td>4.625404</td>\n",
       "      <td>0.229762</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.128062</td>\n",
       "      <td>4.595353</td>\n",
       "      <td>0.237798</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.128778</td>\n",
       "      <td>4.592059</td>\n",
       "      <td>0.227381</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(80, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(stor_path/'models/deep_philosopher_fine_tuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (149 items)\n",
       "x: LMTextList\n",
       "xxbos [ xxmaj xxunk ] enclose xxunk xxunk . xxmaj small · xxunk · enclose material that has been added , but can be xxunk as though it were part of the xxunk xxunk . xxmaj xxunk xxunk , and also xxunk of xxunk that are not xxunk , are meant as xxunk to xxunk the structure of a xxunk or a thought . xxmaj every xxunk - point xxunk . . . . xxunk the xxunk of a xxunk xxunk that seems to present more difficulty than it is xxunk .,xxbos xxmaj in his xxunk for this work , xxmaj descartes is following a xxunk ( started by xxmaj xxunk ) which uses ‘ first xxunk ’ as a xxunk for xxunk .,xxbos xxmaj some xxunk ago i was xxunk by how many false things i had believed , and by how doubtful was the structure of beliefs that i had based on them . i realized that if i wanted to xxunk anything in the xxunk that was xxunk and likely to last , i needed — just once in my life — to xxunk everything completely and start again from the xxunk . xxmaj it looked like an xxunk xxunk , and i xxunk to xxunk until i was old enough to be sure that there was nothing to be xxunk from xxunk it off any longer . i have now xxunk it for so long that i have no xxunk for going on xxunk to do it rather than xxunk to work . xxmaj so today i have set all my xxunk xxunk and xxunk for myself a clear xxunk of free time . i am here quite alone , and at last i will xxunk myself , xxunk and without xxunk back , to xxunk my opinions .,xxbos i can do this without showing that all my beliefs are false , which is xxunk more than i could ever xxunk . xxmaj my reason tells me that as well as xxunk xxunk from xxunk that are obviously xxunk , i should also withhold it from ones that are xxunk completely certain and indubitable . xxmaj so all i need , for the xxunk of xxunk all my opinions , is to find in each of them at least some reason for doubt . i can do this without going through them one by one , which would take xxunk : once the xxunk of a xxunk have been undermined , the rest xxunk of its own xxunk ; so i will go straight for the xxunk principles on which all my former beliefs xxunk .,xxbos xxmaj whatever i have accepted until now as most true has come to me through my senses . xxmaj but xxunk i have found that they have deceived me , and it is xxunk to xxunk completely those who have deceived us even once .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/jupyter/deep-philosopher/app;\n",
       "\n",
       "Valid: LabelList (16 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj finally , i must not complain that xxmaj god xxunk to the acts of will in which i go wrong . xxmaj what there is in these acts that comes from xxmaj god is wholly true and good ; and it is a perfection in me that i can xxunk them . xxmaj falsity and error are xxunk a privation ; and this privation has no need for help from xxmaj god , because it is n’t a thing , a being . xxmaj indeed , when it is considered in xxunk to xxmaj god as its cause , it is n’t really a privation but rather a mere negation . · xxmaj that is , it is a mere fact about something that is not the case ; it does not xxunk the xxunk that it ought to be the case . i ought to xxunk my will when i do n’t understand , but it is n’t true that xxmaj god ought to have xxunk such xxunk on xxunk xxmaj god has given me the freedom to xxunk or not xxunk in cases where he did not give me clear understanding ; he is xxunk not to xxunk for that . xxmaj but i am to xxunk for xxunk that freedom by coming to xxunk on matters that i do n’t xxunk understand . xxmaj of course xxmaj god easily could have xxunk things so that , while xxunk all my freedom and still being limited in what i understand , i never made a xxunk . xxmaj he could do this either by xxunk me a vivid and clear understanding of everything that i was ever likely to think about ; or by xxunk me always to remember that i ought not to form opinions on matters i do n’t vividly and clearly understand . i can see that if xxmaj god had made me this way , i would — considered just in myself , as if nothing else existed — have been more perfect than i actually am . xxmaj but the universe as a whole may have some perfection that xxunk that some parts of it be capable of error while others are not , so that it would be a xxunk universe if all its parts were exactly xxunk · in being xxunk from xxunk i am not xxunk to complain about xxmaj god ’s xxunk me a xxunk role in his xxunk of things · by xxunk me as one of the xxunk that is n’t xxunk from xxunk,xxbos xxunk any part of the body is moved by another part that is some xxunk away , it can be moved in the same xxunk by any of the parts that xxunk in between , without the more xxunk part doing anything . xxmaj for example , in a xxunk xxup xxunk , if one xxunk xxunk is xxunk so that the other xxunk a xxunk , a could have been moved in just the same way if b or xxunk had been xxunk and xxunk had not moved at all . xxmaj similarly , when i feel a pain in my foot , this xxunk by means of xxunk that run from the foot up to the brain . xxmaj when the xxunk are xxunk in the foot , they xxunk on xxunk parts of the brain and make them xxunk ; and nature has xxunk it xxunk that this motion should produce in the mind a sensation of pain as though xxunk in the foot . xxmaj but since these xxunk xxunk from the foot to the brain through the xxunk , the xxunk , the xxunk region , the back and the xxunk , that same sensation of ‘ pain in the foot ’ can come about when one of the xxunk parts is xxunk , even if nothing xxunk in the foot . xxmaj this xxunk xxunk for any other sensation .,xxbos ( 3 ) xxmaj but now , when i am xxunk to know myself and my xxunk better , i do n’t think i should xxunk accept everything i seem to have xxunk from the senses , but i do n’t think i should call it all into doubt .,xxbos a xxunk made xxunk xxunk to the xxunk of its nature in xxunk the wrong time , just as a well made and xxunk xxunk does ; and we might look at the human body in the same way . xxmaj we could see it as a kind of xxunk made up of xxunk , xxunk , xxunk , xxunk , xxunk and xxunk in such a way that , even if there were no mind in it , it would still xxunk exactly as it now does in all the cases where movement is n’t xxunk the xxunk of the will or , therefore , of the mind . xxmaj if such a body xxunk from xxunk [ a xxunk in which xxunk xxunk of xxunk xxunk in the body ] , for example , and is xxunk by the xxunk of the xxunk that xxunk produces in the mind a sensation of thirst , that will xxunk the xxunk and other bodily parts in such a way as to xxunk the body to take a drink , which will make the xxunk xxunk . xxmaj yet this is as natural as a xxunk body ’s being xxunk by a xxunk xxunk of the xxunk to take a drink that is good for it . · xxmaj in a way , we might say , it is not xxunk xxmaj just as we could say that a xxunk that xxunk xxunk is ‘ xxunk from its nature ’ , we might say that the xxunk body that takes a xxunk drink is ‘ xxunk from its nature ’ , that is , from the xxunk of movements that usually occur in human bodies . xxmaj but that involves using ‘ nature ’ as a way of xxunk one thing with another — a xxunk man with a xxunk one , a xxunk made xxunk with an xxunk one — whereas i have been using ‘ nature ’ not to make xxunk but to speak of what can be found in the things themselves ; and this xxunk is xxunk .,xxbos xxmaj size , shape , position and so on are well known and xxunk to me as general kinds of xxunk , but there are also countless particular xxunk xxunk them that i perceive when i xxunk to them . xxmaj the truths about all these matters are so open to me , and so much in xxunk with my nature , that when i first discover any of them it xxunk less like xxunk something new than like xxunk something i had known before , or xxunk for the first time something that was already in my mind without my having xxunk my mental xxunk xxunk it .\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/jupyter/deep-philosopher/app;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(856, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(856, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=856, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f60059263b0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/jupyter/deep-philosopher/app/static'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(856, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(856, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=856, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load(stor_path/'models/deep_philosopher_fine_tuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust below parameters to test inference (generated texts)\n",
    "\n",
    "TEXT = \"Life will\"\n",
    "N_WORDS = 60\n",
    "N_SENTENCES = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life will is complete and complete by means of the natural light , and is complete ( as i am awake ) ? That is where the most important things —\n",
      "Life will vividly and clearly is vividly and clearly imply the truth ; innate knowledge and also knowledge of knowledge will mislead on through the senses ; and somehow in imagination it\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS) for _ in range(N_SENTENCES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life will has from experience all this perfections to him — for all his perfections as well as his perfections — not for all of them to have brought it about that there is no he - ( or none that i understand ) between the perfections that i attribute to God . When on the other hand i examine\n"
     ]
    }
   ],
   "source": [
    "pred_raw = learn.predict(TEXT, N_WORDS, temperature=0.8, min_p=0.0001, no_unk=True)\n",
    "print(pred_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Life will has from experience all this perfections to him — for all his perfections as well as his perfections — not for all of them to have brought it about that there is no he - ( or none that i understand ) between the perfections that i attribute to God ']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_raw.split('.')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
